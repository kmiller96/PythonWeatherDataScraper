{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Weather Data With Python\n",
    "Hello everyone! Welcome to another one of my projects. This time I'll be looking into how to scrape data off the internet automatically using Python, specifically BeautifulSoup and urllib2. Although not the most complicated project, as I don't have to manually sift the data but rather just centralise it all, it should still serve as useful for the community. \n",
    "\n",
    "This Jupyter notebook is purely to show the proof-of-concepts for my processes such as requesting, downloading and sorting the data. The full collection of the data will be done by an external python file which has to be executed in the terminal.\n",
    "\n",
    "\n",
    "## Overview Of The Project\n",
    "The project's main aim is to collect weather data from all over Western Australia and centralise it into this repository. In theory this process could be extended to Australia-wide, but I don't want that much data. I'll illustrate how one could do that later in the project. That way others can use it in their own projects without needing to repeat the process I'm about to undertake.\n",
    "\n",
    "The data I've collected is simply the options specified by the Bureau of Meteorology on their [Climate Data database](http://www.bom.gov.au/climate/data/). As such, I'll be collecting information on: Rainfall; Temperature (Max & Min); and Solar Exposure.\n",
    "\n",
    "My general thought process in storing the data will be through the use of a 3D array (known as a [Panel](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Panel.html) in Pandas) with:\n",
    " - Each 'slice' (which forms a 2D matrix) repesenting a station.\n",
    " - Each row representing a date.\n",
    " - Each column representing a piece of data.\n",
    "\n",
    "I've attempted to represent a slice of the data for station $n$ below.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "Date & Rainfall(mm) & ...\\\\\n",
    "01/01/2015 & 12.3 & ...\\\\ \n",
    "02/01/2015 & 6.1 & ...\\\\ \n",
    "... & ... & ...\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## Usage of The Data\n",
    "For anyone reading this notebook who might be interested in using my data that I've collected, I give full permission without any attribution needed. Go forth and solve the world's problems using data!\n",
    "\n",
    "All data has been scraped from [The Bureau of Meteorology](http://www.bom.gov.au). I claim rights to none of the data. As such I recommend reading into their data policies before using it for commercial use (but I'm sure personal use will be fine).\n",
    "\n",
    "---\n",
    "With all of that out of the way, let's begin!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import all of the goodies that I'll be using.\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next, define some global variables that I'll be using.\n",
    "BOM_HOME = r'http://www.bom.gov.au'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Finding The Data\n",
    "The first step in this project will be finding the data to collect. As already mentioned, you can find most of the data online at the [Climate Data database](http://www.bom.gov.au/climate/data/). However, collecting data this way is really tedious as you would have to manually fill in forms. Although this is possible with Python packages such as [Selenum](http://selenium-python.readthedocs.io/) there is definately an easier way.\n",
    "\n",
    "## 1.1. Tinkering With The Portal\n",
    "\n",
    "The first thing I did was to gain some familarity with how the portal worked. So I searched up for a weather station, found it's station number and went to that data page. If you'd like to follow along, I used station 9021.\n",
    "\n",
    "Once I went to the new page, I noticed that the URL was structured like so:\n",
    "\n",
    "> http://www.bom.gov.au/jsp/ncc/cdio/weatherData/av?p_nccObsCode=136&p_display_type=dailyDataFile&p_startYear=&p_c=&p_stn_num=009021\n",
    "\n",
    "So already you can see the URL contains a query string. For the most part I can't really tell what each parameter does, but I did notice that my station number was in the string:\n",
    "\n",
    "> p_stn_num=009021\n",
    "\n",
    "So naturally I played around with this parameter. Of course this enabled me to move to new weather station! Let me illustrate that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the weather station for a given station number\n",
    "def getStationName(station_num):\n",
    "    page = urllib2.urlopen(\n",
    "        r'http://www.bom.gov.au/jsp/ncc/cdio/weatherData/av?p_nccObsCode=136&p_display_type=dailyDataFile&p_startYear=&p_c=&p_stn_num=' \n",
    "        + str(station_num).zfill(6)\n",
    "    )\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    return soup.h2.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station 9021: Perth Airport \n",
      "Station 9022: Guildford Post Office \n"
     ]
    }
   ],
   "source": [
    "print \"Station 9021:\", getStationName(9021)\n",
    "print \"Station 9022:\", getStationName(9022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you explore this further however you'll notice that not all stations exist. Uncomment the code below if you want to see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print getStationName(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to find the numbers of all of the stations in WA. Fortunately that's really easy!\n",
    "\n",
    "## 1.2. Finding The Weather Stations That Exist\n",
    "Although you could simply test all 6-digit numbers and capture any that exist, it is far easier to just do a simply google search. The BOM actually all you to query for a list of the stations [here](http://www.bom.gov.au/climate/data/stations/). At this point I also noticed a lot more data that I could collect if I desired.\n",
    "\n",
    "I also noticed a big problem. You can simply request the data from BOM instead of collecting it manually! For the sake of practice I will however just collect the data manually. \n",
    "\n",
    "[This text file](http://www.bom.gov.au/climate/data/lists_by_element/alphaWA_136.txt) lists all of the weather stations that collect rainfall data in WA. So that means that I can just use this list to collect my data!\n",
    "\n",
    "The text file is structured for humans to read, not a computer to. At this point I used excel to convert the text file into a CSV. After a little bit of cleaning up of the headings and footers, I imported the CSV into a Pandas dataframe so we can access the information easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Site</th>\n",
       "      <th>Name</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Lon</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>Years</th>\n",
       "      <th>%</th>\n",
       "      <th>AWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7118</td>\n",
       "      <td>ABBOTTS</td>\n",
       "      <td>-26.4000</td>\n",
       "      <td>118.4000</td>\n",
       "      <td>Sep 1898</td>\n",
       "      <td>Nov 1913</td>\n",
       "      <td>6.8</td>\n",
       "      <td>45</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10258</td>\n",
       "      <td>ABERVON</td>\n",
       "      <td>-30.7833</td>\n",
       "      <td>117.9833</td>\n",
       "      <td>May 1968</td>\n",
       "      <td>Aug 1973</td>\n",
       "      <td>5.2</td>\n",
       "      <td>97</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4000</td>\n",
       "      <td>ABYDOS</td>\n",
       "      <td>-21.4167</td>\n",
       "      <td>118.9333</td>\n",
       "      <td>Jul 1917</td>\n",
       "      <td>Dec 1974</td>\n",
       "      <td>40.0</td>\n",
       "      <td>70</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4045</td>\n",
       "      <td>ABYDOS WOODSTOCK</td>\n",
       "      <td>-21.6200</td>\n",
       "      <td>118.9550</td>\n",
       "      <td>Apr 1901</td>\n",
       "      <td>Sep 1997</td>\n",
       "      <td>65.2</td>\n",
       "      <td>67</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9971</td>\n",
       "      <td>ACTON PARK</td>\n",
       "      <td>-33.7845</td>\n",
       "      <td>115.4072</td>\n",
       "      <td>Nov 2000</td>\n",
       "      <td>Nov 2017</td>\n",
       "      <td>17.0</td>\n",
       "      <td>98</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Site              Name      Lat       Lon     Start       End  Years   %  \\\n",
       "0   7118           ABBOTTS -26.4000  118.4000  Sep 1898  Nov 1913    6.8  45   \n",
       "1  10258           ABERVON -30.7833  117.9833  May 1968  Aug 1973    5.2  97   \n",
       "2   4000            ABYDOS -21.4167  118.9333  Jul 1917  Dec 1974   40.0  70   \n",
       "3   4045  ABYDOS WOODSTOCK -21.6200  118.9550  Apr 1901  Sep 1997   65.2  67   \n",
       "4   9971        ACTON PARK -33.7845  115.4072  Nov 2000  Nov 2017   17.0  98   \n",
       "\n",
       "   AWS  \n",
       "0    N  \n",
       "1    N  \n",
       "2    N  \n",
       "3    N  \n",
       "4    N  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pandas.read_csv('rainfall_stations_wa.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the titles used in the data are self-explainatory. However, let me explain the last two columns:\n",
    " 1. **Percentage (%)**: After tinkering around with the data, I think it is a measure of the completeness of the data collected. The higher the percentage, the lower the number of missing data points. Obviously 100% indicates that there aren't any missing data points at all during its time of operation.\n",
    " 2. **AWS**: This column signifies that the weather station is an *Automatic Weather Station*. The BOM state that their readings are more consistent and accurate, which would be an interesting observation to back up with data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Downloading Historical Data\n",
    "Now that we know which stations exist, we want to explore if it is possible to automate the downloading process.\n",
    "\n",
    "On each weather station page, there is a button to download all historical data on the page. Thus we want to target this URL using Beautiful Soup. By inspecting the source code on each page, it is contained within the \"downloads\" class as the second item in the list. Let's try to target that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def downloadDataLink(station_num):\n",
    "    page = urllib2.urlopen(\n",
    "        r'http://www.bom.gov.au/jsp/ncc/cdio/weatherData/av?p_nccObsCode=136&p_display_type=dailyDataFile&p_startYear=&p_c=&p_stn_num=' \n",
    "        + str(station_num).zfill(6)\n",
    "    )\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    download_link_extension = soup.find(\n",
    "        'a', \n",
    "        {'title': \"Data file for daily rainfall data for all years\"}\n",
    "    )['href']\n",
    "    return str(BOM_HOME + download_link_extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know that's a little complicated looking. Let me break it down for you:\n",
    " 1. Read the station data page. This is simply where you get to if you manaully input in the station number into the portal. The zfill is to ensure that the number is formatted correctly.\n",
    " 2. Get the HTML code from Beautiful Soup.\n",
    " 3. Extract the download link. You can find it by targeting the correct title.\n",
    " 4. The link is only appended to the base BOM link. So return the full link by concatinating the two strings together.\n",
    " \n",
    "You can see how this output looks for different stations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.bom.gov.au/jsp/ncc/cdio/weatherData/av?p_display_type=dailyZippedDataFile&p_stn_num=009021&p_c=-16486220&p_nccObsCode=136&p_startYear=2017\n",
      "http://www.bom.gov.au/jsp/ncc/cdio/weatherData/av?p_display_type=dailyZippedDataFile&p_stn_num=009022&p_c=-16489829&p_nccObsCode=136&p_startYear=1954\n"
     ]
    }
   ],
   "source": [
    "print downloadDataLink(9021)\n",
    "print downloadDataLink(9022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the data from here is actually really simple. If you open the URL using urllib2 you'll receive the entire source code, which you can simply save by writing that code into a zipped file.\n",
    "\n",
    "I've done this below to show how this could be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = urllib2.urlopen(downloadDataLink(9021))\n",
    "with open('station_9021.zip', 'wb') as zipfile:\n",
    "    zipfile.write(data.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
